import { existsSync, mkdirSync } from "node:fs";
import { writeFile } from "node:fs/promises";
import * as p from "@clack/prompts";
import { HttpCrawler, PlaywrightCrawler, log, purgeDefaultStorages } from "crawlee";
import { htmlToMarkdown } from "mdream";
import { generateLlmsTxtArtifacts } from "mdream/llms-txt";
import { withMinimalPreset } from "mdream/preset/minimal";
import { dirname, join, normalize, resolve } from "pathe";
import { withHttps } from "ufo";
import picomatch from "picomatch";
import { extractionPlugin } from "mdream/plugins";

//#region src/glob-utils.ts
/**
* Parse a URL that may contain glob patterns
* Example: https://nuxtseo.com/docs/** -> { baseUrl: "https://nuxtseo.com", pattern: "/docs/**", isGlob: true }
*/
function parseUrlPattern(input) {
	if (!(input.includes("*") || input.includes("?") || input.includes("["))) return {
		baseUrl: input,
		pattern: "",
		isGlob: false
	};
	try {
		const urlWithoutGlob = (input.startsWith("http") ? input : `https://${input}`).replace(/\*.*$/, "");
		const url = new URL(urlWithoutGlob);
		const baseUrl = `${url.protocol}//${url.host}`;
		const patternStart = input.indexOf(url.host) + url.host.length;
		return {
			baseUrl,
			pattern: input.substring(patternStart),
			isGlob: true
		};
	} catch {
		throw new Error(`Invalid URL pattern: "${input}". Please provide a valid URL with glob patterns (e.g., "example.com/docs/*" or "https://example.com/api/**").`);
	}
}
/**
* Check if a URL matches a glob pattern
*/
function matchesGlobPattern(url, parsedPattern) {
	if (!parsedPattern.isGlob) return true;
	try {
		const urlObj = new URL(url);
		const urlPath = urlObj.pathname + urlObj.search + urlObj.hash;
		if (`${urlObj.protocol}//${urlObj.host}` !== parsedPattern.baseUrl) return false;
		let pattern = parsedPattern.pattern;
		if (pattern.endsWith("*") && !pattern.endsWith("**") && !pattern.endsWith("/*")) {
			const base = pattern.slice(0, -1);
			pattern = `{${base},${base}/**}`;
		}
		return picomatch(pattern)(urlPath);
	} catch {
		return false;
	}
}
/**
* Get the starting URL for crawling from a glob pattern
* For https://nuxtseo.com/docs/**, we want to start at https://nuxtseo.com
*/
function getStartingUrl(parsedPattern) {
	if (!parsedPattern.isGlob) return withHttps(parsedPattern.baseUrl);
	const pattern = parsedPattern.pattern;
	const firstGlobIndex = pattern.search(/[*?[]/);
	if (firstGlobIndex === -1) return withHttps(parsedPattern.baseUrl + pattern);
	const beforeGlob = pattern.substring(0, firstGlobIndex);
	const lastSlash = beforeGlob.lastIndexOf("/");
	const pathBeforeGlob = lastSlash >= 0 ? beforeGlob.substring(0, lastSlash + 1) : "/";
	return withHttps(parsedPattern.baseUrl + pathBeforeGlob);
}
/**
* Check if a URL should be excluded based on exclude patterns
*/
function isUrlExcluded(url, excludePatterns) {
	if (!excludePatterns || excludePatterns.length === 0) return false;
	try {
		const urlObj = new URL(url);
		const urlPath = urlObj.pathname + urlObj.search + urlObj.hash;
		return excludePatterns.some((pattern) => {
			if (pattern.includes("://")) {
				const parsedPattern = parseUrlPattern(pattern);
				if (parsedPattern.isGlob) return matchesGlobPattern(url, parsedPattern);
				return url === pattern;
			}
			if (pattern.startsWith("/")) return picomatch(pattern.endsWith("/*") ? pattern.replace("/*", "/**") : pattern)(urlPath);
			return picomatch(pattern)(urlPath) || picomatch(pattern)(urlPath.substring(1));
		});
	} catch {
		return false;
	}
}
/**
* Validate glob pattern syntax
*/
function validateGlobPattern(pattern) {
	try {
		parseUrlPattern(pattern);
		return;
	} catch (error) {
		return `Invalid glob pattern: ${error instanceof Error ? error.message : error}`;
	}
}

//#endregion
//#region src/metadata-extractor.ts
function extractMetadata(html, url) {
	const links = [];
	let title = "";
	let description = "";
	let keywords = "";
	let author = "";
	htmlToMarkdown(html, {
		plugins: [extractionPlugin({
			"a[href]": (element) => {
				const href = element.attributes?.href;
				if (href) try {
					const absoluteUrl = new URL(href, url).href;
					if (!links.includes(absoluteUrl)) links.push(absoluteUrl);
				} catch {}
			},
			"title": (element) => {
				if (!title && element.textContent) title = element.textContent.trim();
			},
			"meta[name=\"description\"]": (element) => {
				if (!description && element.attributes?.content) description = element.attributes.content.trim();
			},
			"meta[property=\"og:description\"]": (element) => {
				if (!description && element.attributes?.content) description = element.attributes.content.trim();
			},
			"meta[name=\"keywords\"]": (element) => {
				if (!keywords && element.attributes?.content) keywords = element.attributes.content.trim();
			},
			"meta[name=\"author\"]": (element) => {
				if (!author && element.attributes?.content) author = element.attributes.content.trim();
			},
			"meta[property=\"og:title\"]": (element) => {
				if (!title && element.attributes?.content) title = element.attributes.content.trim();
			}
		})],
		origin: new URL(url).origin
	});
	return {
		title: title || new URL(url).pathname,
		description: description || void 0,
		keywords: keywords || void 0,
		author: author || void 0,
		links: links.filter((link) => {
			try {
				const linkUrl = new URL(link);
				const baseUrl = new URL(url);
				return linkUrl.hostname === baseUrl.hostname;
			} catch {
				return false;
			}
		})
	};
}

//#endregion
//#region src/crawl.ts
async function loadSitemapWithoutRetries(sitemapUrl) {
	const controller = new AbortController();
	const timeoutId = setTimeout(() => controller.abort(), 1e4);
	try {
		const response = await fetch(sitemapUrl, {
			signal: controller.signal,
			headers: { "User-Agent": "mdream-crawler/1.0" }
		});
		clearTimeout(timeoutId);
		if (!response.ok) throw new Error(`Sitemap not found: ${response.status}`);
		const xmlContent = await response.text();
		if (xmlContent.includes("<sitemapindex")) {
			const sitemapIndexRegex = /<sitemap[^>]*>.*?<loc>(.*?)<\/loc>.*?<\/sitemap>/gs;
			const childSitemaps = [];
			let match;
			while (true) {
				match = sitemapIndexRegex.exec(xmlContent);
				if (match === null) break;
				let url = match[1];
				if (url.startsWith("<![CDATA[") && url.endsWith("]]>")) url = url.slice(9, -3);
				childSitemaps.push(url);
			}
			const allUrls = [];
			for (const childSitemapUrl of childSitemaps) try {
				const childUrls = await loadSitemapWithoutRetries(childSitemapUrl);
				allUrls.push(...childUrls);
			} catch (error) {
				console.warn(`Failed to load child sitemap ${childSitemapUrl}:`, error instanceof Error ? error.message : "Unknown error");
			}
			return allUrls;
		} else {
			const urls = [];
			const urlRegex = /<url[^>]*>.*?<loc>(.*?)<\/loc>.*?<\/url>/gs;
			let match;
			while (true) {
				match = urlRegex.exec(xmlContent);
				if (match === null) break;
				let url = match[1];
				if (url.startsWith("<![CDATA[") && url.endsWith("]]>")) url = url.slice(9, -3);
				urls.push(url);
			}
			return urls;
		}
	} catch (error) {
		clearTimeout(timeoutId);
		if (error instanceof Error && error.name === "AbortError") throw new Error("Sitemap request timed out after 10 seconds");
		throw error;
	}
}
async function crawlAndGenerate(options, onProgress) {
	const { urls, outputDir: rawOutputDir, maxRequestsPerCrawl = Number.MAX_SAFE_INTEGER, generateLlmsTxt = true, generateLlmsFullTxt = false, generateIndividualMd = true, origin, driver = "http", useChrome, followLinks = false, maxDepth = 1, globPatterns = [], crawlDelay, exclude = [], siteNameOverride, descriptionOverride, verbose = false, skipSitemap = false, onPage } = options;
	const outputDir = resolve(normalize(rawOutputDir));
	if (verbose) log.setLevel(log.LEVELS.INFO);
	else log.setLevel(log.LEVELS.OFF);
	let patterns;
	try {
		patterns = globPatterns.length > 0 ? globPatterns : urls.map(parseUrlPattern);
	} catch (error) {
		throw new Error(`Invalid URL pattern: ${error instanceof Error ? error.message : "Unknown error"}`);
	}
	let startingUrls = patterns.map(getStartingUrl);
	const progress = {
		sitemap: {
			status: "discovering",
			found: 0,
			processed: 0
		},
		crawling: {
			status: "starting",
			total: 0,
			processed: 0
		},
		generation: { status: "idle" }
	};
	const sitemapAttempts = [];
	if (startingUrls.length > 0 && !skipSitemap) {
		const baseUrl = new URL(startingUrls[0]).origin;
		const homePageUrl = baseUrl;
		onProgress?.(progress);
		const robotsUrl = new URL("/robots.txt", baseUrl).toString();
		const robotsController = new AbortController();
		const robotsTimeoutId = setTimeout(() => robotsController.abort(), 1e4);
		let robotsResponse;
		try {
			robotsResponse = await fetch(robotsUrl, {
				signal: robotsController.signal,
				headers: { "User-Agent": "mdream-crawler/1.0" }
			});
			clearTimeout(robotsTimeoutId);
		} catch (error) {
			clearTimeout(robotsTimeoutId);
			robotsResponse = null;
		}
		if (robotsResponse?.ok) {
			const sitemapMatches = (await robotsResponse.text()).match(/Sitemap:\s*(.*)/gi);
			if (sitemapMatches && sitemapMatches.length > 0) {
				progress.sitemap.found = sitemapMatches.length;
				progress.sitemap.status = "processing";
				onProgress?.(progress);
				const robotsSitemaps = sitemapMatches.map((match) => match.replace(/Sitemap:\s*/i, "").trim());
				for (const sitemapUrl of robotsSitemaps) try {
					const robotsUrls = await loadSitemapWithoutRetries(sitemapUrl);
					sitemapAttempts.push({
						url: sitemapUrl,
						success: true
					});
					if (patterns.some((p$1) => p$1.isGlob)) {
						const filteredUrls = robotsUrls.filter((url) => {
							return !isUrlExcluded(url, exclude) && patterns.some((pattern) => matchesGlobPattern(url, pattern));
						});
						startingUrls = filteredUrls;
						progress.sitemap.processed = filteredUrls.length;
						onProgress?.(progress);
						break;
					} else {
						const filteredUrls = robotsUrls.filter((url) => {
							return !isUrlExcluded(url, exclude);
						});
						if (filteredUrls.length > 0) {
							startingUrls = filteredUrls;
							progress.sitemap.processed = filteredUrls.length;
							onProgress?.(progress);
							break;
						}
					}
				} catch (error) {
					sitemapAttempts.push({
						url: sitemapUrl,
						success: false,
						error: error instanceof Error ? error.message : "Unknown error"
					});
				}
			}
		}
		let mainSitemapProcessed = false;
		const mainSitemapUrl = `${baseUrl}/sitemap.xml`;
		try {
			const sitemapUrls = await loadSitemapWithoutRetries(mainSitemapUrl);
			sitemapAttempts.push({
				url: mainSitemapUrl,
				success: true
			});
			if (patterns.some((p$1) => p$1.isGlob)) {
				const filteredUrls = sitemapUrls.filter((url) => {
					return !isUrlExcluded(url, exclude) && patterns.some((pattern) => matchesGlobPattern(url, pattern));
				});
				startingUrls = filteredUrls;
				progress.sitemap.found = sitemapUrls.length;
				progress.sitemap.processed = filteredUrls.length;
				onProgress?.(progress);
				mainSitemapProcessed = true;
			} else {
				const filteredUrls = sitemapUrls.filter((url) => {
					return !isUrlExcluded(url, exclude);
				});
				if (filteredUrls.length > 0) {
					startingUrls = filteredUrls;
					progress.sitemap.found = sitemapUrls.length;
					progress.sitemap.processed = filteredUrls.length;
					onProgress?.(progress);
					mainSitemapProcessed = true;
				}
			}
		} catch (error) {
			sitemapAttempts.push({
				url: mainSitemapUrl,
				success: false,
				error: error instanceof Error ? error.message : "Unknown error"
			});
			if (!mainSitemapProcessed) {
				const commonSitemaps = [
					`${baseUrl}/sitemap_index.xml`,
					`${baseUrl}/sitemaps.xml`,
					`${baseUrl}/sitemap-index.xml`
				];
				for (const sitemapUrl of commonSitemaps) try {
					const altUrls = await loadSitemapWithoutRetries(sitemapUrl);
					sitemapAttempts.push({
						url: sitemapUrl,
						success: true
					});
					if (patterns.some((p$1) => p$1.isGlob)) {
						const filteredUrls = altUrls.filter((url) => {
							return !isUrlExcluded(url, exclude) && patterns.some((pattern) => matchesGlobPattern(url, pattern));
						});
						startingUrls = filteredUrls;
						progress.sitemap.found = altUrls.length;
						progress.sitemap.processed = filteredUrls.length;
						onProgress?.(progress);
						break;
					} else {
						const filteredUrls = altUrls.filter((url) => {
							return !isUrlExcluded(url, exclude);
						});
						if (filteredUrls.length > 0) {
							startingUrls = filteredUrls;
							progress.sitemap.found = altUrls.length;
							progress.sitemap.processed = filteredUrls.length;
							onProgress?.(progress);
							break;
						}
					}
				} catch (error$1) {
					sitemapAttempts.push({
						url: sitemapUrl,
						success: false,
						error: error$1 instanceof Error ? error$1.message : "Unknown error"
					});
				}
			}
		}
		const successfulSitemaps = sitemapAttempts.filter((a) => a.success);
		const failedSitemaps = sitemapAttempts.filter((a) => !a.success);
		if (successfulSitemaps.length > 0) {
			const sitemapUrl = successfulSitemaps[0].url;
			if (progress.sitemap.processed > 0) p.note(`Found sitemap at ${sitemapUrl} with ${progress.sitemap.processed} URLs`, "Sitemap Discovery");
			else p.note(`Found sitemap at ${sitemapUrl} but no URLs matched your search criteria`, "Sitemap Discovery");
		} else if (failedSitemaps.length > 0) {
			const firstAttempt = failedSitemaps[0];
			if (firstAttempt.error?.includes("404")) p.note(`No sitemap found, using crawler to discover pages`, "Sitemap Discovery");
			else p.note(`Could not access sitemap: ${firstAttempt.error}`, "Sitemap Discovery");
		}
		if (!startingUrls.includes(homePageUrl)) startingUrls.unshift(homePageUrl);
		progress.sitemap.status = "completed";
		progress.crawling.total = startingUrls.length;
		onProgress?.(progress);
	} else if (skipSitemap && startingUrls.length > 0) {
		progress.sitemap.status = "completed";
		progress.sitemap.found = 0;
		progress.sitemap.processed = 0;
		progress.crawling.total = startingUrls.length;
		onProgress?.(progress);
	}
	if (!existsSync(outputDir)) mkdirSync(outputDir, { recursive: true });
	const results = [];
	const processedUrls = /* @__PURE__ */ new Set();
	const shouldCrawlUrl = (url) => {
		if (isUrlExcluded(url, exclude)) return false;
		if (!patterns.some((p$1) => p$1.isGlob)) return true;
		return patterns.some((pattern) => matchesGlobPattern(url, pattern));
	};
	const createRequestHandler = (crawlerType) => {
		return async ({ request, body, page, enqueueLinks, response }) => {
			const startTime = Date.now();
			progress.crawling.currentUrl = request.loadedUrl;
			onProgress?.(progress);
			if (response?.statusCode && (response.statusCode < 200 || response.statusCode >= 300)) return;
			const homePageUrl = new URL(startingUrls[0]).origin;
			let html;
			let title;
			if (crawlerType === "playwright") {
				await page.waitForLoadState("networkidle");
				title = await page.title();
				html = await page.innerHTML("html");
			} else {
				html = typeof body === "string" ? body : body.toString();
				title = "";
			}
			const metadata = extractMetadata(html, request.loadedUrl);
			if (!title) title = metadata.title;
			const shouldProcessMarkdown = shouldCrawlUrl(request.loadedUrl);
			const pageOrigin = origin || new URL(request.loadedUrl).origin;
			if (onPage && shouldProcessMarkdown) await onPage({
				url: request.loadedUrl,
				html,
				title,
				metadata,
				origin: pageOrigin
			});
			let md = "";
			if (shouldProcessMarkdown && (!onPage || generateIndividualMd)) md = htmlToMarkdown(html, withMinimalPreset({ origin: pageOrigin }));
			let filePath;
			if (shouldProcessMarkdown && generateIndividualMd) {
				const urlObj = new URL(request.loadedUrl);
				const safeSegments = (urlObj.pathname === "/" ? "/index" : urlObj.pathname).replace(/\/$/, "").split("/").filter((seg) => seg.length > 0).map((seg) => seg.replace(/[^\w\-]/g, "-"));
				filePath = join(outputDir, normalize(`${safeSegments.length > 0 ? safeSegments.join("/") : "index"}.md`));
				const fileDir = dirname(filePath);
				if (fileDir && !existsSync(fileDir)) mkdirSync(fileDir, { recursive: true });
				await writeFile(filePath, md, "utf-8");
			}
			const isHomePage = request.loadedUrl.replace(/\/$/, "") === homePageUrl.replace(/\/$/, "");
			if (shouldProcessMarkdown || isHomePage) {
				const result = {
					url: request.loadedUrl,
					title,
					content: md,
					filePath: shouldProcessMarkdown ? filePath : void 0,
					timestamp: startTime,
					success: true,
					metadata,
					depth: request.userData?.depth || 0
				};
				results.push(result);
				progress.crawling.processed = results.length;
				onProgress?.(progress);
			}
			if (followLinks && (request.userData?.depth || 0) < maxDepth) {
				const currentDepth = (request.userData?.depth || 0) + 1;
				const filteredLinks = metadata.links.filter((link) => {
					return shouldCrawlUrl(link);
				});
				if (enqueueLinks) await enqueueLinks({
					urls: filteredLinks,
					userData: { depth: currentDepth }
				});
				else for (const link of filteredLinks) if (!processedUrls.has(link)) processedUrls.add(link);
			}
		};
	};
	let crawler;
	const crawlerOptions = {
		requestHandler: createRequestHandler(driver),
		errorHandler: async ({ request, response, error }) => {
			if (verbose) console.error(`[ERROR] URL: ${request.url}, Status: ${response?.statusCode || "N/A"}, Error: ${error?.message || "Unknown"}`);
			if (response?.statusCode && response?.statusCode >= 400) {
				request.noRetry = true;
				const result = {
					url: request.url,
					title: "",
					content: "",
					timestamp: Date.now(),
					success: false,
					error: `HTTP ${response.statusCode}`,
					metadata: {
						title: "",
						description: "",
						links: []
					},
					depth: request.userData?.depth || 0
				};
				results.push(result);
			} else if (error) {
				request.noRetry = true;
				const result = {
					url: request.url,
					title: "",
					content: "",
					timestamp: Date.now(),
					success: false,
					error: error.message || "Unknown error",
					metadata: {
						title: "",
						description: "",
						links: []
					},
					depth: request.userData?.depth || 0
				};
				results.push(result);
			}
		},
		maxRequestsPerCrawl,
		respectRobotsTxtFile: false
	};
	if (crawlDelay) crawlerOptions.requestHandlerTimeoutSecs = crawlDelay;
	if (driver === "playwright") {
		const playwrightOptions = crawlerOptions;
		if (useChrome) playwrightOptions.launchContext = {
			...playwrightOptions.launchContext,
			useChrome
		};
		crawler = new PlaywrightCrawler(playwrightOptions);
	} else crawler = new HttpCrawler(crawlerOptions);
	const initialRequests = startingUrls.map((url) => ({
		url,
		userData: { depth: 0 }
	}));
	progress.crawling.status = "processing";
	progress.crawling.total = startingUrls.length;
	onProgress?.(progress);
	try {
		await crawler.run(initialRequests);
	} catch (error) {
		if (verbose) {
			console.error(`[CRAWLER ERROR] ${error instanceof Error ? error.message : "Unknown error"}`);
			console.error(`[CRAWLER ERROR] Stack trace:`, error instanceof Error ? error.stack : "No stack trace");
		}
		throw error;
	}
	progress.crawling.status = "completed";
	onProgress?.(progress);
	if (results.some((r) => r.success)) {
		progress.generation.status = "generating";
		onProgress?.(progress);
		const successfulResults = results.filter((r) => r.success);
		const firstUrl = new URL(withHttps(urls[0]));
		const origin$1 = firstUrl.origin;
		const homePageResult = successfulResults.find((r) => {
			const resultUrl = new URL(withHttps(r.url));
			return resultUrl.href === origin$1 || resultUrl.href === `${origin$1}/`;
		});
		const siteName = siteNameOverride || homePageResult?.metadata?.title || homePageResult?.title || firstUrl.hostname;
		const description = descriptionOverride || homePageResult?.metadata?.description || successfulResults[0]?.metadata?.description;
		if (generateLlmsTxt || generateLlmsFullTxt) {
			progress.generation.current = "Generating llms.txt files";
			onProgress?.(progress);
			const contentResults = successfulResults.filter((result) => {
				if (!result.content) return false;
				return result.content.trim().replace(/^---\s*\n(?:.*\n)*?---\s*/, "").trim().length > 10;
			});
			const seenUrls = /* @__PURE__ */ new Set();
			const llmsResult = await generateLlmsTxtArtifacts({
				files: contentResults.filter((result) => {
					if (seenUrls.has(result.url)) return false;
					seenUrls.add(result.url);
					return true;
				}).map((result) => ({
					filePath: result.filePath,
					title: result.title,
					content: result.content,
					url: result.url,
					metadata: result.metadata
				})),
				siteName,
				description,
				origin: origin$1 || firstUrl.origin,
				generateFull: generateLlmsFullTxt,
				outputDir
			});
			if (generateLlmsTxt) {
				progress.generation.current = "Writing llms.txt";
				onProgress?.(progress);
				await writeFile(join(outputDir, "llms.txt"), llmsResult.llmsTxt, "utf-8");
			}
			if (generateLlmsFullTxt && llmsResult.llmsFullTxt) {
				progress.generation.current = "Writing llms-full.txt";
				onProgress?.(progress);
				await writeFile(join(outputDir, "llms-full.txt"), llmsResult.llmsFullTxt, "utf-8");
			}
		}
		progress.generation.status = "completed";
		onProgress?.(progress);
	}
	await purgeDefaultStorages();
	return results;
}

//#endregion
export { parseUrlPattern as n, validateGlobPattern as r, crawlAndGenerate as t };