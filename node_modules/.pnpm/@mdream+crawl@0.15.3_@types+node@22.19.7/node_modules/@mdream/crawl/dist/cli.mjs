import { n as parseUrlPattern, r as validateGlobPattern, t as crawlAndGenerate } from "./_chunks/crawl.mjs";
import { accessSync, constants, mkdirSync, readFileSync, unlinkSync, writeFileSync } from "node:fs";
import * as p from "@clack/prompts";
import { PlaywrightCrawler } from "crawlee";
import { dirname, join, resolve } from "pathe";
import { withHttps } from "ufo";
import { fileURLToPath } from "node:url";
import { addDependency } from "nypm";

//#region src/playwright-utils.ts
async function checkPlaywrightInstallation() {
	try {
		await import("playwright");
		return true;
	} catch {
		return false;
	}
}
async function promptPlaywrightInstall() {
	const shouldInstall = await p.confirm({
		message: "Playwright is required for the Playwright driver. Install it now?",
		initialValue: true
	});
	if (p.isCancel(shouldInstall) || !shouldInstall) return false;
	const s = p.spinner();
	s.start("Installing Playwright globally...");
	try {
		await addDependency("playwright", { global: true });
		s.stop("Playwright installed successfully!");
		return true;
	} catch (fallbackError) {
		s.stop("Failed to install Playwright");
		p.log.error(`Installation failed: ${fallbackError}`);
		return false;
	}
}
async function ensurePlaywrightInstalled() {
	if (await checkPlaywrightInstallation()) return true;
	p.log.warn("Playwright driver selected but Playwright is not installed.");
	if (!await promptPlaywrightInstall()) {
		p.log.error("Cannot proceed with Playwright driver without Playwright installed.");
		return false;
	}
	return true;
}
async function isUseChromeSupported() {
	try {
		const crawler = new PlaywrightCrawler({
			launchContext: { useChrome: true },
			requestHandler: async () => {},
			maxRequestsPerCrawl: 1
		});
		const page = await crawler.browserPool.newPage();
		await page.evaluate(() => {
			return window.navigator.userAgent;
		});
		await page.close();
		await crawler.browserPool.closeAllBrowsers();
		crawler.stop();
		return true;
	} catch {}
	return false;
}

//#endregion
//#region src/cli.ts
const packageJsonPath = join(dirname(fileURLToPath(import.meta.url)), "..", "package.json");
const version = JSON.parse(readFileSync(packageJsonPath, "utf-8")).version;
function checkOutputDirectoryPermissions(outputDir) {
	try {
		mkdirSync(outputDir, { recursive: true });
		accessSync(outputDir, constants.W_OK);
		const testFile = join(outputDir, ".mdream-test");
		try {
			writeFileSync(testFile, "test");
			unlinkSync(testFile);
		} catch (err) {
			return {
				success: false,
				error: `Cannot write to output directory: ${err instanceof Error ? err.message : "Unknown error"}`
			};
		}
		return { success: true };
	} catch (err) {
		if (err instanceof Error) {
			if (err.message.includes("EACCES")) return {
				success: false,
				error: `Permission denied: Cannot write to output directory '${outputDir}'. Please check permissions or run with appropriate privileges.`
			};
			return {
				success: false,
				error: `Failed to access output directory: ${err.message}`
			};
		}
		return {
			success: false,
			error: "Failed to access output directory"
		};
	}
}
async function interactiveCrawl() {
	console.clear();
	p.intro(`‚òÅÔ∏è  @mdream/crawl v${version}`);
	const urlsInput = await p.text({
		message: "Enter starting URL for crawling (supports glob patterns):",
		placeholder: "e.g. docs.example.com, site.com/docs/**",
		validate: (value) => {
			if (!value) return "Please enter at least one URL";
			const urls$1 = value.split(",").map((url) => url.trim());
			for (const url of urls$1) {
				const globError = validateGlobPattern(url);
				if (globError) return globError;
				try {
					if (!parseUrlPattern(url).isGlob) try {
						new URL(withHttps(url));
					} catch {
						return `Invalid URL: ${withHttps(url)}`;
					}
				} catch (error) {
					return error instanceof Error ? error.message : "Invalid URL pattern";
				}
			}
		}
	});
	if (p.isCancel(urlsInput)) {
		p.cancel("Operation cancelled.");
		return null;
	}
	const urls = urlsInput.split(",").map((url) => url.trim());
	let globPatterns;
	try {
		globPatterns = urls.map(parseUrlPattern);
	} catch (error) {
		p.cancel(error instanceof Error ? error.message : "Invalid URL pattern");
		return null;
	}
	const outputDir = "output";
	const crawlerOptions = await p.group({ driver: () => p.select({
		message: "Select crawler driver:",
		options: [{
			value: "http",
			label: "HTTP Crawler (Fast, for static content)",
			hint: "Recommended"
		}, {
			value: "playwright",
			label: "Playwright (Slower, supports JavaScript)"
		}],
		initialValue: "http"
	}) }, { onCancel: () => {
		p.cancel("Operation cancelled.");
		process.exit(0);
	} });
	const advancedOptions = await p.group({
		outputFormats: () => p.multiselect({
			message: "Select output formats:",
			options: [
				{
					value: "llms.txt",
					label: "llms.txt (basic format)",
					hint: "Recommended"
				},
				{
					value: "llms-full.txt",
					label: "llms-full.txt (extended format)"
				},
				{
					value: "markdown",
					label: "Individual Markdown files"
				}
			],
			initialValues: [
				"llms.txt",
				"llms-full.txt",
				"markdown"
			]
		}),
		skipSitemap: () => p.confirm({
			message: "Skip sitemap.xml and robots.txt discovery?",
			initialValue: false
		})
	}, { onCancel: () => {
		p.cancel("Operation cancelled.");
		process.exit(0);
	} });
	const firstUrl = urls[0];
	const inferredOrigin = (() => {
		try {
			const url = new URL(withHttps(firstUrl));
			return `${url.protocol}//${url.host}`;
		} catch {
			return;
		}
	})();
	const outputFormats = advancedOptions.outputFormats.map((f) => {
		switch (f) {
			case "llms.txt": return "llms.txt";
			case "llms-full.txt": return "llms-full.txt";
			case "markdown": return "Individual MD files";
			default: return f;
		}
	});
	const summary = [
		`URLs: ${urls.join(", ")}`,
		`Output: ${outputDir}`,
		`Driver: ${crawlerOptions.driver}`,
		`Max pages: Unlimited`,
		`Follow links: Yes (depth 3)`,
		`Output formats: ${outputFormats.join(", ")}`,
		`Sitemap discovery: ${advancedOptions.skipSitemap ? "Skipped" : "Automatic"}`,
		inferredOrigin && `Origin: ${inferredOrigin}`
	].filter(Boolean);
	p.note(summary.join("\n"), "Crawl Configuration");
	if (advancedOptions.skipSitemap && globPatterns.some((p$1) => p$1.isGlob)) p.log.warn("Warning: Using --skip-sitemap with glob URLs may not discover all matching pages.");
	return {
		urls,
		outputDir: resolve(outputDir),
		driver: crawlerOptions.driver,
		maxRequestsPerCrawl: Number.MAX_SAFE_INTEGER,
		followLinks: true,
		generateLlmsTxt: advancedOptions.outputFormats.includes("llms.txt"),
		generateLlmsFullTxt: advancedOptions.outputFormats.includes("llms-full.txt"),
		generateIndividualMd: advancedOptions.outputFormats.includes("markdown"),
		origin: inferredOrigin,
		globPatterns,
		verbose: false,
		maxDepth: 3,
		skipSitemap: advancedOptions.skipSitemap
	};
}
async function showCrawlResults(successful, failed, outputDir, generatedFiles, durationSeconds) {
	const messages = [];
	const durationStr = `${durationSeconds.toFixed(1)}s`;
	const stats = failed > 0 ? `${successful} pages, ${failed} failed` : `${successful} pages`;
	messages.push(`üìÑ ${stats} ‚Ä¢ ‚è±Ô∏è  ${durationStr}`);
	messages.push(`üì¶ ${generatedFiles.join(", ")}`);
	messages.push(`üìÅ ${outputDir}`);
	p.note(messages.join("\n"), "‚úÖ Complete");
}
function parseCliArgs() {
	const args = process.argv.slice(2);
	if (args.includes("--help") || args.includes("-h")) {
		console.log(`
@mdream/crawl v${version}

Multi-page website crawler that generates comprehensive llms.txt files

Usage:
  @mdream/crawl [options] <url>             Crawl a website with CLI flags
  @mdream/crawl                             Start interactive mode

Options:
  -u, --url <url>              Website URL to crawl
  -o, --output <dir>           Output directory (default: output)
  -d, --depth <number>         Crawl depth (default: 3)
  --driver <http|playwright>   Crawler driver (default: http)
  --artifacts <list>           Comma-separated list of artifacts: llms.txt,llms-full.txt,markdown (default: all)
  --origin <url>               Origin URL for resolving relative paths (overrides auto-detection)
  --site-name <name>           Override site name (overrides auto-extracted title)
  --description <desc>         Override site description (overrides auto-extracted description)
  --max-pages <number>        Maximum pages to crawl (default: unlimited)
  --crawl-delay <seconds>     Crawl delay in seconds
  --exclude <pattern>         Exclude URLs matching glob patterns (can be used multiple times)
  --skip-sitemap              Skip sitemap.xml and robots.txt discovery
  -v, --verbose               Enable verbose logging
  -h, --help                  Show this help message
  --version                   Show version number

Note: Sitemap discovery and robots.txt checking are automatic unless --skip-sitemap is used.

Examples:
  @mdream/crawl -u harlanzw.com --artifacts "llms.txt,markdown"
  @mdream/crawl --url https://docs.example.com --depth 2 --artifacts "llms-full.txt"
  @mdream/crawl -u example.com --exclude "*/admin/*" --exclude "*/api/*"
  @mdream/crawl -u example.com --verbose
  @mdream/crawl -u example.com --skip-sitemap
`);
		process.exit(0);
	}
	if (args.includes("--version")) {
		console.log(`@mdream/crawl v${version}`);
		process.exit(0);
	}
	if (args.length === 0) return null;
	const getArgValue = (flag) => {
		const index = args.findIndex((arg) => arg === flag || arg === flag.replace("--", "-"));
		return index >= 0 && index + 1 < args.length ? args[index + 1] : void 0;
	};
	const getArgValues = (flag) => {
		const values = [];
		for (let i = 0; i < args.length; i++) if (args[i] === flag || args[i] === flag.replace("--", "-")) {
			if (i + 1 < args.length && !args[i + 1].startsWith("-")) values.push(args[i + 1]);
		}
		return values;
	};
	const urlFromFlag = getArgValue("--url") || getArgValue("-u");
	const urlFromArgs = args.find((arg) => !arg.startsWith("-") && !args[args.indexOf(arg) - 1]?.startsWith("-"));
	const url = urlFromFlag || urlFromArgs;
	if (!url) {
		p.log.error("Error: URL is required when using CLI arguments");
		p.log.info("Use --help for usage information or run without arguments for interactive mode");
		process.exit(1);
	}
	const globError = validateGlobPattern(url);
	if (globError) {
		p.log.error(`Error: ${globError}`);
		process.exit(1);
	}
	let parsed;
	try {
		parsed = parseUrlPattern(url);
	} catch (error) {
		p.log.error(`Error: ${error instanceof Error ? error.message : "Invalid URL pattern"}`);
		process.exit(1);
	}
	if (!parsed.isGlob) try {
		new URL(withHttps(url));
	} catch {
		p.log.error(`Error: Invalid URL: ${withHttps(url)}`);
		process.exit(1);
	}
	const excludePatterns = getArgValues("--exclude");
	for (const pattern of excludePatterns) {
		const excludeError = validateGlobPattern(pattern);
		if (excludeError) {
			p.log.error(`Error in exclude pattern: ${excludeError}`);
			process.exit(1);
		}
	}
	const depthStr = getArgValue("--depth") || getArgValue("-d") || "3";
	const depth = Number.parseInt(depthStr);
	if (Number.isNaN(depth) || depth < 1 || depth > 10) {
		p.log.error("Error: Depth must be between 1 and 10");
		process.exit(1);
	}
	const driver = getArgValue("--driver");
	if (driver && driver !== "http" && driver !== "playwright") {
		p.log.error("Error: Driver must be either \"http\" or \"playwright\"");
		process.exit(1);
	}
	const maxPagesStr = getArgValue("--max-pages");
	if (maxPagesStr) {
		const maxPages = Number.parseInt(maxPagesStr);
		if (Number.isNaN(maxPages) || maxPages < 1) {
			p.log.error("Error: Max pages must be a positive number");
			process.exit(1);
		}
	}
	const crawlDelayStr = getArgValue("--crawl-delay");
	if (crawlDelayStr) {
		const crawlDelay = Number.parseInt(crawlDelayStr);
		if (Number.isNaN(crawlDelay) || crawlDelay < 0) {
			p.log.error("Error: Crawl delay must be a non-negative number");
			process.exit(1);
		}
	}
	const artifactsStr = getArgValue("--artifacts");
	const artifacts = artifactsStr ? artifactsStr.split(",").map((a) => a.trim()) : [
		"llms.txt",
		"llms-full.txt",
		"markdown"
	];
	const validArtifacts = [
		"llms.txt",
		"llms-full.txt",
		"markdown"
	];
	for (const artifact of artifacts) if (!validArtifacts.includes(artifact)) {
		p.log.error(`Error: Invalid artifact '${artifact}'. Valid options: ${validArtifacts.join(", ")}`);
		process.exit(1);
	}
	const originOverride = getArgValue("--origin");
	const inferredOrigin = (() => {
		if (originOverride) return originOverride;
		try {
			const urlObj = new URL(withHttps(url));
			return `${urlObj.protocol}//${urlObj.host}`;
		} catch {
			return;
		}
	})();
	const siteNameOverride = getArgValue("--site-name");
	const descriptionOverride = getArgValue("--description");
	const patterns = [parsed];
	const verbose = args.includes("--verbose") || args.includes("-v");
	const skipSitemap = args.includes("--skip-sitemap");
	if (skipSitemap && parsed.isGlob) p.log.warn("Warning: Using --skip-sitemap with glob URLs may not discover all matching pages.");
	return {
		urls: [url],
		outputDir: resolve(getArgValue("--output") || getArgValue("-o") || "output"),
		driver: driver || "http",
		maxRequestsPerCrawl: Number.parseInt(maxPagesStr || String(Number.MAX_SAFE_INTEGER)),
		followLinks: true,
		maxDepth: depth,
		generateLlmsTxt: artifacts.includes("llms.txt"),
		generateLlmsFullTxt: artifacts.includes("llms-full.txt"),
		generateIndividualMd: artifacts.includes("markdown"),
		siteNameOverride,
		descriptionOverride,
		origin: inferredOrigin,
		globPatterns: patterns,
		crawlDelay: crawlDelayStr ? Number.parseInt(crawlDelayStr) : void 0,
		exclude: excludePatterns.length > 0 ? excludePatterns : void 0,
		verbose,
		skipSitemap
	};
}
async function main() {
	const cliOptions = parseCliArgs();
	let options;
	if (cliOptions) {
		options = cliOptions;
		p.intro(`‚òÅÔ∏è  mdream v${version}`);
		const formats = [];
		if (options.generateLlmsTxt) formats.push("llms.txt");
		if (options.generateLlmsFullTxt) formats.push("llms-full.txt");
		if (options.generateIndividualMd) formats.push("Individual MD files");
		const summary = [
			`URL: ${options.urls.join(", ")}`,
			`Output: ${options.outputDir}`,
			`Driver: ${options.driver}`,
			`Depth: ${options.maxDepth}`,
			`Formats: ${formats.join(", ")}`,
			options.exclude && options.exclude.length > 0 && `Exclude: ${options.exclude.join(", ")}`,
			options.skipSitemap && `Skip sitemap: Yes`,
			options.verbose && `Verbose: Enabled`
		].filter(Boolean);
		p.note(summary.join("\n"), "Configuration");
	} else options = await interactiveCrawl();
	if (!options) process.exit(0);
	const permCheck = checkOutputDirectoryPermissions(options.outputDir);
	if (!permCheck.success) {
		p.log.error(permCheck.error);
		if (permCheck.error?.includes("Permission denied")) p.log.info("Tip: Try running with elevated privileges (e.g., sudo) or change the output directory permissions.");
		process.exit(1);
	}
	if (options.driver === "playwright") if (await isUseChromeSupported()) {
		options.useChrome = true;
		p.log.info("System Chrome detected and enabled.");
	} else {
		if (!await ensurePlaywrightInstalled()) {
			p.log.error("Cannot proceed without Playwright. Please install it manually or use the HTTP driver instead.");
			process.exit(1);
		}
		p.log.info("Using global playwright instance.");
	}
	const s = p.spinner();
	s.start("Starting crawl...");
	const startTime = Date.now();
	const results = await crawlAndGenerate(options, (progress) => {
		if (progress.sitemap.status === "discovering") s.message("Discovering sitemaps...");
		else if (progress.sitemap.status === "processing") s.message(`Processing sitemap... Found ${progress.sitemap.found} URLs`);
		else if (progress.crawling.status === "processing") {
			const processedCount = progress.crawling.processed;
			const totalCount = progress.crawling.total;
			const currentUrl = progress.crawling.currentUrl;
			if (currentUrl) {
				const shortUrl = currentUrl.length > 60 ? `${currentUrl.substring(0, 57)}...` : currentUrl;
				if (processedCount > totalCount) s.message(`Crawling ${processedCount}: ${shortUrl}`);
				else s.message(`Crawling ${processedCount}/${totalCount}: ${shortUrl}`);
			} else if (processedCount > totalCount) s.message(`Crawling... ${processedCount} pages`);
			else s.message(`Crawling... ${processedCount}/${totalCount} pages`);
		} else if (progress.generation.status === "generating") {
			const current = progress.generation.current || "Generating files";
			s.message(current);
		}
	});
	s.stop();
	const durationSeconds = (Date.now() - startTime) / 1e3;
	const successful = results.filter((r) => r.success).length;
	const failed = results.filter((r) => !r.success).length;
	const failedResults = results.filter((r) => !r.success);
	if (failed > 0 && cliOptions) {
		p.log.error("Failed URLs:");
		failedResults.forEach((result) => {
			p.log.error(`  ${result.url}: ${result.error || "Unknown error"}`);
		});
	} else if (failed > 0) {
		console.log("\nFailed URLs:");
		failedResults.forEach((result) => {
			console.log(`  - ${result.url}: ${result.error || "Unknown error"}`);
		});
	}
	const generatedFiles = [];
	if (successful > 0) {
		if (options.generateLlmsTxt) generatedFiles.push("llms.txt");
		if (options.generateLlmsFullTxt) generatedFiles.push("llms-full.txt");
		if (options.generateIndividualMd) generatedFiles.push(`${successful} MD files`);
	}
	await showCrawlResults(successful, failed, options.outputDir, generatedFiles, durationSeconds);
	process.exit(0);
}
main().catch((error) => {
	p.log.error(`Unexpected error: ${error}`);
	process.exit(1);
});

//#endregion
export {  };