{"version":3,"file":"index.mjs","names":["join"],"sources":["../src/index.ts"],"sourcesContent":["import type { FetchedDoc, SkillConfig, SkillResult } from './types'\nimport { existsSync, mkdirSync, writeFileSync } from 'node:fs'\nimport { join } from 'node:path'\n\nexport type { FetchedDoc, SkillConfig, SkillResult }\n\n/**\n * Generate a skill from a documentation site\n */\nexport async function generateSkill(\n  config: SkillConfig,\n  onProgress?: (info: { url: string, count: number, phase: 'fetch' | 'index' }) => void,\n): Promise<SkillResult> {\n  const {\n    url,\n    outputDir = '.skilld',\n    chunkSize = 1000,\n    chunkOverlap = 200,\n    maxPages = 100,\n    skipLlmsTxt = false,\n    model = 'Xenova/bge-small-en-v1.5',\n  } = config\n\n  const siteName = getSiteName(url)\n  const skillDir = join(outputDir, siteName)\n  const referencesDir = join(skillDir, 'references')\n  const dbPath = join(skillDir, 'search.db')\n\n  mkdirSync(referencesDir, { recursive: true })\n\n  // Fetch docs\n  let docs: FetchedDoc[]\n  let skillContent: string | undefined\n\n  if (!skipLlmsTxt) {\n    const llmsResult = await fetchFromLlmsTxt(url, maxPages, onProgress)\n    if (llmsResult) {\n      docs = llmsResult.docs\n      skillContent = llmsResult.llmsContent\n    }\n    else {\n      docs = await crawlSite(url, maxPages, onProgress)\n    }\n  }\n  else {\n    docs = await crawlSite(url, maxPages, onProgress)\n  }\n\n  if (docs.length === 0) {\n    throw new Error('No documents found to index')\n  }\n\n  // Write SKILL.md\n  const skillPath = join(skillDir, 'SKILL.md')\n  if (skillContent) {\n    writeFileSync(skillPath, skillContent)\n  }\n\n  // Chunk and index docs\n  const { splitText } = await import('./split-text')\n  const { sqliteVec } = await import('retriv/db/sqlite-vec')\n  const { transformers } = await import('retriv/embeddings/transformers')\n\n  const documents: Array<{ id: string, content: string, metadata: Record<string, any> }> = []\n\n  for (const doc of docs) {\n    const chunks = splitText(doc.content, { chunkSize, chunkOverlap })\n\n    for (const chunk of chunks) {\n      const section = extractSection(chunk.text)\n      const docId = chunks.length > 1\n        ? `${doc.url}#chunk-${chunk.index}`\n        : doc.url\n\n      // Prepend title/section for better semantic matching\n      const prefix = [doc.title, section].filter(Boolean).join(' > ')\n      const content = prefix ? `${prefix}\\n\\n${chunk.text}` : chunk.text\n\n      documents.push({\n        id: docId,\n        content,\n        metadata: {\n          source: doc.url,\n          title: doc.title,\n          ...(section && { section }),\n          ...(chunks.length > 1 && {\n            chunkIndex: chunk.index,\n            chunkTotal: chunks.length,\n          }),\n        },\n      })\n\n      // Write reference file\n      const filename = sanitizeFilename(docId) + '.md'\n      const refPath = join(referencesDir, filename)\n      writeFileSync(refPath, formatReferenceFile(docId, doc, section, chunk, chunks.length))\n    }\n  }\n\n  // Index with retriv\n  onProgress?.({ url: 'embedding', count: documents.length, phase: 'index' })\n\n  const db = await sqliteVec({\n    path: dbPath,\n    embeddings: transformers({ model }),\n  })\n\n  await db.index(documents)\n  await db.close?.()\n\n  return {\n    siteName,\n    skillPath,\n    referencesDir,\n    dbPath,\n    chunkCount: documents.length,\n  }\n}\n\nfunction getSiteName(url: string): string {\n  const urlObj = new URL(url)\n  return urlObj.hostname.replace(/^www\\./, '')\n}\n\nfunction sanitizeFilename(id: string): string {\n  return id\n    .replace(/^https?:\\/\\//, '')\n    .replace(/[#?]/g, '-')\n    .replace(/[^a-z0-9.-]/gi, '-')\n    .replace(/-+/g, '-')\n    .replace(/^-|-$/g, '')\n    .slice(0, 100)\n}\n\nfunction extractSection(text: string): string | undefined {\n  const headings: string[] = []\n  for (const line of text.split('\\n')) {\n    const match = line.match(/^(#{1,6}) ([^\\n]+)$/)\n    if (match) {\n      const level = match[1]!.length\n      const heading = match[2]!.trim()\n      headings.length = level - 1\n      headings[level - 1] = heading\n    }\n  }\n  const section = headings.filter(Boolean).join(' > ')\n  return section || undefined\n}\n\nfunction formatReferenceFile(\n  docId: string,\n  doc: FetchedDoc,\n  section: string | undefined,\n  chunk: { text: string, index: number },\n  totalChunks: number,\n): string {\n  const frontmatter = [\n    '---',\n    `id: \"${docId}\"`,\n    `source: \"${doc.url}\"`,\n    `title: \"${doc.title}\"`,\n  ]\n  if (section)\n    frontmatter.push(`section: \"${section}\"`)\n  if (totalChunks > 1)\n    frontmatter.push(`chunk: ${chunk.index + 1}/${totalChunks}`)\n  frontmatter.push('---', '')\n\n  const prefix = [doc.title, section].filter(Boolean).join(' > ')\n  return frontmatter.join('\\n') + (prefix ? `${prefix}\\n\\n` : '') + chunk.text\n}\n\nasync function fetchFromLlmsTxt(\n  baseUrl: string,\n  maxPages: number,\n  onProgress?: (info: { url: string, count: number, phase: 'fetch' | 'index' }) => void,\n): Promise<{ docs: FetchedDoc[], llmsContent: string } | null> {\n  const urlObj = new URL(baseUrl)\n  const llmsUrl = `${urlObj.origin}/llms.txt`\n\n  const res = await fetch(llmsUrl, {\n    headers: { 'User-Agent': 'skilld/1.0' },\n  }).catch(() => null)\n\n  if (!res?.ok)\n    return null\n\n  const llmsContent = await res.text()\n  if (llmsContent.length < 50)\n    return null\n\n  // Parse markdown links\n  const links = parseLinks(llmsContent)\n  const docs: FetchedDoc[] = []\n\n  let count = 0\n  for (const { title, url } of links.slice(0, maxPages)) {\n    count++\n    onProgress?.({ url, count, phase: 'fetch' })\n\n    // Resolve relative URLs\n    const absoluteUrl = url.startsWith('http') ? url : new URL(url, urlObj.origin).href\n    const content = await fetchMarkdown(absoluteUrl)\n    if (content && content.length >= 50) {\n      docs.push({ url: absoluteUrl, title, content })\n    }\n  }\n\n  return { docs, llmsContent }\n}\n\nfunction parseLinks(content: string): Array<{ title: string, url: string }> {\n  const links: Array<{ title: string, url: string }> = []\n  const linkRegex = /\\[([^\\]]+)\\]\\(([^)]+)\\)/g\n  let match\n\n  while ((match = linkRegex.exec(content)) !== null) {\n    const [, title, url] = match\n    if (url.includes('/raw/') || url.endsWith('.md')) {\n      links.push({ title, url })\n    }\n  }\n\n  return links\n}\n\nasync function fetchMarkdown(url: string): Promise<string | null> {\n  const res = await fetch(url, {\n    headers: { 'User-Agent': 'skilld/1.0' },\n  }).catch(() => null)\n\n  if (!res?.ok)\n    return null\n\n  return res.text()\n}\n\nasync function crawlSite(\n  url: string,\n  maxPages: number,\n  onProgress?: (info: { url: string, count: number, phase: 'fetch' | 'index' }) => void,\n): Promise<FetchedDoc[]> {\n  const { htmlToMarkdown } = await import('mdream')\n  const { crawlAndGenerate } = await import('@mdream/crawl')\n  const { tmpdir } = await import('node:os')\n  const { join } = await import('node:path')\n\n  const docs: FetchedDoc[] = []\n  let count = 0\n  const outputDir = join(tmpdir(), `skilld-crawl-${Date.now()}`)\n\n  await crawlAndGenerate({\n    urls: [url],\n    outputDir,\n    maxRequestsPerCrawl: maxPages,\n    followLinks: true,\n    onPage: async ({ url: pageUrl, html, title }) => {\n      count++\n      onProgress?.({ url: pageUrl, count, phase: 'fetch' })\n\n      const urlObj = new URL(pageUrl)\n      const markdown = htmlToMarkdown(html, { origin: urlObj.origin })\n\n      if (markdown && markdown.length >= 50) {\n        docs.push({ url: pageUrl, title: title || pageUrl, content: markdown })\n      }\n    },\n  })\n\n  return docs\n}\n"],"mappings":";;AASA,eAAsB,cACpB,QACA,YACsB;CACtB,MAAM,EACJ,KACA,YAAY,WACZ,YAAY,KACZ,eAAe,KACf,WAAW,KACX,cAAc,OACd,QAAQ,+BACN;CAEJ,MAAM,WAAW,YAAY,IAAI;CACjC,MAAM,WAAWA,OAAK,WAAW,SAAS;CAC1C,MAAM,gBAAgBA,OAAK,UAAU,aAAa;CAClD,MAAM,SAASA,OAAK,UAAU,YAAY;AAE1C,WAAU,eAAe,EAAE,WAAW,MAAM,CAAC;CAG7C,IAAI;CACJ,IAAI;AAEJ,KAAI,CAAC,aAAa;EAChB,MAAM,aAAa,MAAM,iBAAiB,KAAK,UAAU,WAAW;AACpE,MAAI,YAAY;AACd,UAAO,WAAW;AAClB,kBAAe,WAAW;QAG1B,QAAO,MAAM,UAAU,KAAK,UAAU,WAAW;OAInD,QAAO,MAAM,UAAU,KAAK,UAAU,WAAW;AAGnD,KAAI,KAAK,WAAW,EAClB,OAAM,IAAI,MAAM,8BAA8B;CAIhD,MAAM,YAAYA,OAAK,UAAU,WAAW;AAC5C,KAAI,aACF,eAAc,WAAW,aAAa;CAIxC,MAAM,EAAE,cAAc,MAAM,OAAO;CACnC,MAAM,EAAE,cAAc,MAAM,OAAO;CACnC,MAAM,EAAE,iBAAiB,MAAM,OAAO;CAEtC,MAAM,YAAmF,EAAE;AAE3F,MAAK,MAAM,OAAO,MAAM;EACtB,MAAM,SAAS,UAAU,IAAI,SAAS;GAAE;GAAW;GAAc,CAAC;AAElE,OAAK,MAAM,SAAS,QAAQ;GAC1B,MAAM,UAAU,eAAe,MAAM,KAAK;GAC1C,MAAM,QAAQ,OAAO,SAAS,IAC1B,GAAG,IAAI,IAAI,SAAS,MAAM,UAC1B,IAAI;GAGR,MAAM,SAAS,CAAC,IAAI,OAAO,QAAQ,CAAC,OAAO,QAAQ,CAAC,KAAK,MAAM;GAC/D,MAAM,UAAU,SAAS,GAAG,OAAO,MAAM,MAAM,SAAS,MAAM;AAE9D,aAAU,KAAK;IACb,IAAI;IACJ;IACA,UAAU;KACR,QAAQ,IAAI;KACZ,OAAO,IAAI;KACX,GAAI,WAAW,EAAE,SAAS;KAC1B,GAAI,OAAO,SAAS,KAAK;MACvB,YAAY,MAAM;MAClB,YAAY,OAAO;MACpB;KACF;IACF,CAAC;AAKF,iBADgBA,OAAK,eADJ,iBAAiB,MAAM,GAAG,MACE,EACtB,oBAAoB,OAAO,KAAK,SAAS,OAAO,OAAO,OAAO,CAAC;;;AAK1F,cAAa;EAAE,KAAK;EAAa,OAAO,UAAU;EAAQ,OAAO;EAAS,CAAC;CAE3E,MAAM,KAAK,MAAM,UAAU;EACzB,MAAM;EACN,YAAY,aAAa,EAAE,OAAO,CAAC;EACpC,CAAC;AAEF,OAAM,GAAG,MAAM,UAAU;AACzB,OAAM,GAAG,SAAS;AAElB,QAAO;EACL;EACA;EACA;EACA;EACA,YAAY,UAAU;EACvB;;AAGH,SAAS,YAAY,KAAqB;AAExC,QADe,IAAI,IAAI,IAAI,CACb,SAAS,QAAQ,UAAU,GAAG;;AAG9C,SAAS,iBAAiB,IAAoB;AAC5C,QAAO,GACJ,QAAQ,gBAAgB,GAAG,CAC3B,QAAQ,SAAS,IAAI,CACrB,QAAQ,iBAAiB,IAAI,CAC7B,QAAQ,OAAO,IAAI,CACnB,QAAQ,UAAU,GAAG,CACrB,MAAM,GAAG,IAAI;;AAGlB,SAAS,eAAe,MAAkC;CACxD,MAAM,WAAqB,EAAE;AAC7B,MAAK,MAAM,QAAQ,KAAK,MAAM,KAAK,EAAE;EACnC,MAAM,QAAQ,KAAK,MAAM,sBAAsB;AAC/C,MAAI,OAAO;GACT,MAAM,QAAQ,MAAM,GAAI;GACxB,MAAM,UAAU,MAAM,GAAI,MAAM;AAChC,YAAS,SAAS,QAAQ;AAC1B,YAAS,QAAQ,KAAK;;;AAI1B,QADgB,SAAS,OAAO,QAAQ,CAAC,KAAK,MAAM,IAClC,KAAA;;AAGpB,SAAS,oBACP,OACA,KACA,SACA,OACA,aACQ;CACR,MAAM,cAAc;EAClB;EACA,QAAQ,MAAM;EACd,YAAY,IAAI,IAAI;EACpB,WAAW,IAAI,MAAM;EACtB;AACD,KAAI,QACF,aAAY,KAAK,aAAa,QAAQ,GAAG;AAC3C,KAAI,cAAc,EAChB,aAAY,KAAK,UAAU,MAAM,QAAQ,EAAE,GAAG,cAAc;AAC9D,aAAY,KAAK,OAAO,GAAG;CAE3B,MAAM,SAAS,CAAC,IAAI,OAAO,QAAQ,CAAC,OAAO,QAAQ,CAAC,KAAK,MAAM;AAC/D,QAAO,YAAY,KAAK,KAAK,IAAI,SAAS,GAAG,OAAO,QAAQ,MAAM,MAAM;;AAG1E,eAAe,iBACb,SACA,UACA,YAC6D;CAC7D,MAAM,SAAS,IAAI,IAAI,QAAQ;CAC/B,MAAM,UAAU,GAAG,OAAO,OAAO;CAEjC,MAAM,MAAM,MAAM,MAAM,SAAS,EAC/B,SAAS,EAAE,cAAc,cAAc,EACxC,CAAC,CAAC,YAAY,KAAK;AAEpB,KAAI,CAAC,KAAK,GACR,QAAO;CAET,MAAM,cAAc,MAAM,IAAI,MAAM;AACpC,KAAI,YAAY,SAAS,GACvB,QAAO;CAGT,MAAM,QAAQ,WAAW,YAAY;CACrC,MAAM,OAAqB,EAAE;CAE7B,IAAI,QAAQ;AACZ,MAAK,MAAM,EAAE,OAAO,SAAS,MAAM,MAAM,GAAG,SAAS,EAAE;AACrD;AACA,eAAa;GAAE;GAAK;GAAO,OAAO;GAAS,CAAC;EAG5C,MAAM,cAAc,IAAI,WAAW,OAAO,GAAG,MAAM,IAAI,IAAI,KAAK,OAAO,OAAO,CAAC;EAC/E,MAAM,UAAU,MAAM,cAAc,YAAY;AAChD,MAAI,WAAW,QAAQ,UAAU,GAC/B,MAAK,KAAK;GAAE,KAAK;GAAa;GAAO;GAAS,CAAC;;AAInD,QAAO;EAAE;EAAM;EAAa;;AAG9B,SAAS,WAAW,SAAwD;CAC1E,MAAM,QAA+C,EAAE;CACvD,MAAM,YAAY;CAClB,IAAI;AAEJ,SAAQ,QAAQ,UAAU,KAAK,QAAQ,MAAM,MAAM;EACjD,MAAM,GAAG,OAAO,OAAO;AACvB,MAAI,IAAI,SAAS,QAAQ,IAAI,IAAI,SAAS,MAAM,CAC9C,OAAM,KAAK;GAAE;GAAO;GAAK,CAAC;;AAI9B,QAAO;;AAGT,eAAe,cAAc,KAAqC;CAChE,MAAM,MAAM,MAAM,MAAM,KAAK,EAC3B,SAAS,EAAE,cAAc,cAAc,EACxC,CAAC,CAAC,YAAY,KAAK;AAEpB,KAAI,CAAC,KAAK,GACR,QAAO;AAET,QAAO,IAAI,MAAM;;AAGnB,eAAe,UACb,KACA,UACA,YACuB;CACvB,MAAM,EAAE,mBAAmB,MAAM,OAAO;CACxC,MAAM,EAAE,qBAAqB,MAAM,OAAO;CAC1C,MAAM,EAAE,WAAW,MAAM,OAAO;CAChC,MAAM,EAAE,SAAS,MAAM,OAAO;CAE9B,MAAM,OAAqB,EAAE;CAC7B,IAAI,QAAQ;CACZ,MAAM,YAAY,KAAK,QAAQ,EAAE,gBAAgB,KAAK,KAAK,GAAG;AAE9D,OAAM,iBAAiB;EACrB,MAAM,CAAC,IAAI;EACX;EACA,qBAAqB;EACrB,aAAa;EACb,QAAQ,OAAO,EAAE,KAAK,SAAS,MAAM,YAAY;AAC/C;AACA,gBAAa;IAAE,KAAK;IAAS;IAAO,OAAO;IAAS,CAAC;GAGrD,MAAM,WAAW,eAAe,MAAM,EAAE,QADzB,IAAI,IAAI,QAAQ,CACwB,QAAQ,CAAC;AAEhE,OAAI,YAAY,SAAS,UAAU,GACjC,MAAK,KAAK;IAAE,KAAK;IAAS,OAAO,SAAS;IAAS,SAAS;IAAU,CAAC;;EAG5E,CAAC;AAEF,QAAO"}